import DemoPatternImage from '../dither/DemoPatternImage';
import DemoOrdered from '../dither/DemoOrdered';
// import DemoGammaGradient from './DemoGammaGradient';
// import DemoOrderedColor from './DemoOrderedColor';
// import DemoOrderedPair from './DemoOrderedPair';
// import PaletteSwatches from './PaletteSwatches';
import ImageGrid from '../dither/ImageGrid';

In the [first part](/articles/dither) of this series, we learned what dithering is, why we might want to use it, and a few different ways to apply dithering to an image. So far we've only seen pure black and white implementations. In this article we'll dive deeper into proper implementation of dithering algorithms, taking a look at some issues we skipped over last time and the color versions of these algorithms.

The basics of dithering are well covered in the previous article, so we'll be picking up where we left off. If you haven't read the first part, I highly recommend you check it out before reading this one. That said, let's start by looking into something I intentionally left out of part one:

## Gamma correction

Perceptive readers might have noticed one of the interactive demos in the first article looked a bit different from the rest:

<DemoPatternImage />

This example, used to explain the principle behind [ordered dithering](/articles/dither#ordered-dithering), looks very bright and “washed out”. That's because it doesn't have *gamma correction*, a process necessary to make dithered images accurate which I snuck into every other demo, neglecting to mention it for the sake of simplicity. This one skips it in order to keep a 1-to-1 mapping between the brightness level of a “tile” and the pattern applied to it. With gamma correction applied, the same demo looks like this:

<DemoPatternImage gamma />

So what *is* gamma correction, and why do we need it to make dithered images accurate?

### How it works

The reason we need gamma correction when dithering has to do with color mixing, and the way monitors work, which is in turn a consequence of another little quirk in the way our eyes themselves work.

In short, the human eye has a *non-linear response* to light: we're better at perceiving small changes in brightness in darker places than overall brighter ones. What this means is we don't see twice as much light as twice the brightness—in fact, what we see as a “50% gray” actually reflects only about 20% of incoming light.

Monitors benefit from encoding color in the same non-linear way: brightness becomes *perceptually uniform*, which makes working with color much easier, and out of all the possible brightness levels (256 for most screens), more are dedicated to the darker tones where we can better spot small changes in brightness.

This non-linear encoding is usually achieved by means of a power function, with brightness values between 0 and 1. This makes the midtones darker, while keeping black and white values the same:

**gamma curve demo**

The exponent of this power function is called *gamma*. Most screens use a gamma value between 2.2 and 2.4, which gives an output of about 20% brightness for an input of 50%.

### Dithering and gamma correction

When dithering, we need to account for this curve: since we're telling the monitor to draw only black and white pixels rather than gray, we have to provide the right mix ourselves. That means if we want a 50% gray, about 20% of the pixels should be white, not half of them.

The demo below lets you adjust the gamma value used for dithering a simple black-to-white gradient:

{/* <DemoGammaGradient /> */}

With a gamma of 1.0, or no gamma correction, the dithered gradient looks brighter than the original. Try adjusting gamma until both gradients look roughly the same. The value should be close to your display's gamma setting.

To better understand the effect on dithered images, you can play around with the gamma setting in the demo below, which uses ordered dithering.

<DemoOrdered type="bayer" gammaSlider />

## Color dithering

So far, we've explored dithering techniques for converting images into two-tone black and white. This saves us a lot of complexity when looking at the different methods used for dithering images, but it's fairly limited in what in can do. What happens if we want to dither color images? How about displaying images as best we can in a limited, arbitrary color palette, such as the 16-color palettes often found in old school PCs?

This is where color dithering comes in. Every method we've seen so far can be adapted to work with color images, with varying degrees of complexity to achieve this. In the rest of this article, we'll cover the different methods for dithering in color.

### The easy way

The easiest way to convert black-and-white dithering techniques to work in color is by applying the same process independently on the three color channels: red, green, and blue. This works for any dithering algorithm and requires almost no modification. We can easily convert our ordered dither algorithm to work in color this way:
```
number[n, m] thresholdMap // Threshold map of size n * m
// Initialize threshold map with some values

for (x, y) in input
  color = input[x, y]
  threshold = thresholdMap[x % n, y % m]

  output[x, y].r = if (color.r < threshold) 0; else 1
  output[x, y].g = if (color.g < threshold) 0; else 1
  output[x, y].b = if (color.b < threshold) 0; else 1
```

You can see this algorithm in action below:

{/* <DemoOrderedColor type="bayer" imageUrl="/content/articles/dither2/car-original" /> */}

For such a simple adaptation, the result is pretty good. This method has one huge limitation, though: as it works by thresholding each channel separately, we have no control over how it handles color. Only eight colors are supported: black, white, red, green, blue, cyan, magenta and yellow. The resulting image can only ever display these colors. This causes a number of issues:

- We have very little control over the result, and no way to adapt the color palette to better fit the image.
- These colors are very saturated, making most images very noisy (large areas of gray and other desaturated colors are hit hardest).
- Because the output colors cannot be changed, this method is useless if the intended output device can only display certain colors, for instance on vintage computers (or if we want to use a specific color palette to emulate their aesthetic).

What happens if we want more control over color? For instance, if we want to dither an image using multiple shades of gray, or a specific color palette? Things get a whole lot more complicated is what. Let's take a look at some of the ways we can do this.

## Arbitrary color dithering

Dithering images to an arbitrary color palette is really just the general case, of which the black/white dithering we've been doing is a particular special case. Specifically, one where the palette is made up of two colors, which we'll now define more precisely as their RGB representations: `(0, 0, 0)` and `(1, 1, 1)`.

This may seem obvious, but thinking about it this way will help us realize how to define the general case. When doing black and white dithering, we started from a threshold function to decide which color each pixel should be: if the brightness of that pixel was below 0.5, we painted it black. If it was above, white. To put it another way, the threshold function is a way to check *which color in our palette the original pixel was closest to*, black or white.

This last part is key to understanding the process. Let's take our original thresholding code, leaving dithering aside just for a bit, but this time assuming the input and output are in color:
```
for (x, y) in input
  color = input[x, y]
  output[x, y] =
    if (brightness(color) < 0.5) (0, 0, 0)
    else (1, 1, 1)
```

Now that we know the brightness threshold is just a way of checking whether the color is closer to black or white (how brightness is actually calculated isn't relevant right now), we can generalize the code a bit:
```
palette = [(0, 0, 0), (1, 1, 1)]

for (x, y) in input
  color = input[x, y]
  output[x, y] = findClosestFromPalette(color, palette)
```

This code will do the same as before, only now it's ready for any color palette. What we need to be able to implement this, then, is to figure out what's going on inside the `findClosestFromPalette` function. For black and white, as we've seen, a simple threshold check will do, but that isn't the case for any palette. We'll need a way to find the closest color from a list, and that means a way to find the *distance* between colors.

### Comparing colors

Finding the "distance" between two colors is actually fairly straightforward: we can think of colors as points in a three-dimensional space (those dimensions being red, green, and blue) and apply the formula for distance between two points in 3d space:
```
dr = color1.r - color2.r
dg = color1.g - color2.g
db = color1.b - color2.b

dist = √(dr² + dg² + db²)
```

There are better ways to calculate distance between colors than this, like using a perceptually uniform color space such as CIELab, but the simple RGB vector distance calculation is good enough for our purposes.

### Error diffusion

Let's apply what we've learned to one of the dithering techniques we learned in the last part. For the examples that follow, we'll be using this palette:

{/* <PaletteSwatches palette={[
  '#080000',
  '#201A0B',
  '#432817',
  '#492910',
  '#234309',
  '#5D4F1E',
  '#9C6B20',
  '#A9220F',
  '#2B347C',
  '#2B7409',
  '#D0CA40',
  '#E8A077',
  '#6A94AB',
  '#D5C4B3',
  '#FCE76E',
  '#FCFAE2',
]} /> */}

Error diffusion is the easiest algorithm to convert for arbitrary color palettes, since the way it does color comparisons and calculates error is fairly straightforward. We can esentially just "plug in" our `findClosestFromPalette` function:
```
// For the sake of brevity, this pseudocode assumes we can add and subtract
// colors. All we're doing is adding or subtracting the red, green and blue
// components and saving them to a new color.

color[pal_size] palette = [...]
color[x, y] errorMap // Error for each pixel, initialized to zero

for (x, y) in input
  color = input[x, y] + errorMap[x, y]
  output[x, y] = findClosestFromPalette(color, palette)

  error = color - output[x, y]

  // Error diffusion
  ...
```

And just like that, it works! This is what error diffusion with an arbitrary palette looks like when using a Floyd-Steinberg pattern:

<ImageGrid minSize='30ch' pixelated images={[
  {
    src: '/content/articles/dither2/car-original',
    formats: ['webp', 'jpeg'],
    alt: 'Original image of a yellow car',
    caption: 'Original image'
  },
  {
    src: '/content/articles/dither2/car-ed-fs',
    formats: ['webp', 'png'],
    alt: 'Floyd-Steinberg',
    caption: 'Floyd-Steinberg color dithering'
  },
]} />

This is a pretty good result for a very easy to implement method. As we've seen in the previous post, however, error diffusion has a number of issues, some particularly problematic with small palettes like this one. It can make some weird looking patterns, and doesn't work at all for animations. That's where ordered dithering comes in.

And this is where things get complicated. With error diffusion, converting it to work on an arbitrary palette was only a matter of changing the color comparing function, which we could do easily because error diffusion doesn't rely on any particular method of comparing colors.

Ordered dithering is built around thresholds, so we can't do that. Instead, we'll need to find a way to make our color comparison work around a threshold. There's a few ways we can go about this:

### Color pair thresholding

This is the easiest and most naïve approach, which I'll call *color pair thresholding* because that's exactly what we'll do: a threshold only works on two colors, so for each pixel we'll first find whichever combination of colors from our palette is closest to the original in which ratio, and then apply a threshold between those two.

This is easier to understand with an example. Imagine we have a flat orange color, and our palette consists of red, yellow and blue. We determine that the mix of palette colors closest to the orange is 75% yellow and 25% red, so we pick those two colors and set the ratio at 25%—anything below that is red, anything above is yellow. Then we apply a regular threshold map.

In code, that looks something like this:
```
color[pal_size] palette = [...]
number[n, m] thresholdMap = [...] // Threshold map of size n * m

for (x, y) in input
  color = input[x, y]
  threshold = thresholdMap[x % n, y % m]

  mix = findClosestMix(palette, color)
  output[x, y] = if (mix.ratio < threshold) mix.color1; else mix.color2
```

The `findClosestMix` function should return two colors and a ratio between them. Note that the two colors aren't necessarily the two closest to the pixel's original color, rather whichever two colors become closer when mixed at a given ratio. To find the best mix, we have to try every possibility. Fortunately for us, there isn't an infinite number of possible mix ratios: it's limited to the size of the threshold map. For an 8x8 threshold map that's 64 different mix ratios. Here's pseudocode for `findClosestMix`:
```
function findClosestMix(palette, color, mapSize)
  let bestMix
  let bestMixDistance

  for (color1 in palette)
    for (color2 in palette)
      for (ratio in [0...mapSize - 1])
        mixColor = color1 * (ratio / mapSize) + color2 * (1 - ratio / mapSize)
        mixDistance = distance(mixColor, color)
        if (mixDistance < bestMixDistance)
          bestMixDistance = mixDistance
          bestMix = {
            color1,
            color2,
            ratio
          }
  
  return bestMix
```

This is what the process looks like when applied to an image:

{/* <DemoOrderedPair type="bayer" imageUrl="/content/articles/dither2/car-original" /> */}

Not great. It's mixing very different colors together and we're getting all kinds of weird patterns and artifacts. This happens because, while we told our program to find whichever mix is closest to the original color, we didn't say anything about that mix actually looking good—for instance, avoiding a mix of two very different colors. Without such a restriction, the program might try, for example, to approximate a dark blue by mixing bright cyan and black, rather than using a slightly diffrent shade of blue already in the palette. The result will be mathematically correct, but it will look bad.

What we can do to solve this issue is to change our mix comparison function to penalize a mix of colors very distant from each other. So our code becomes:
```
...
colorDistance = distance(color1, color2)
mixDistance = distance(mixColor, targetColor) + k * colorDistance
...
```

Here, we introduced a parameter `k` to control how much the mix distance factors in. We'll call this parameter `variance`. The higher it is, the more the program will tend to use colors closer to each other. In this example, `variance` is an exponential scale: an increase of 1 in the value halves the penalty for mixing distant colors. The ideal value depends highly on the image and the palette being used. In this case, the best results seem to be between `3.0` and `4.0`:

{/* <DemoOrderedPair
  type="bayer"
  imageUrl="/content/articles/dither2/car-original"
  useVariance
/> */}

This looks better, but the penalties are a bit too harsh and we're getting lots of "blobs" of solid color. We can work around this by reducing the penalty for mixes closer to 50/50 (a ratio of 0.5), thus favoring them over solid colors:
```
...
colorDistance = distance(color1, color2)
rmh = abs(ratio / mapSize - 0.5)
mixDistance = distance(mixColor, targetColor) + k * colorDistance * (0.5 + rmh)
...
```

The change is most noticeable with lower variance, where the result looks significantly better:

{/* <DemoOrderedPair
  type="bayer"
  imageUrl="/content/articles/dither2/car-original"
  useVariance
  useRatio
/> */}

We're getting somewhere, but there's still a long way to go. Low variance results in very obvious color banding and spots, and high variance produces a very noisy image. The colors aren't very accurate to the original image, either. The results are nowhere near as good as we could get with error diffusion, and the limiting factor is only being able to mix two colors at a time.

If we want to improve on this, we need to get rid of that limitation, and that means changing the way we're thinking about threshold maps. A threshold map is just a grayscale image, a two-dimensional data array containing values between 0 and 255. So far we've been using those values as thresholds, but they're just numbers — we can choose to use them for anything we want.

### Pattern dithering

This is a variation on the dithering algorithm named *pattern dithering* used by Adobe Photoshop when saving images with a limited palette, among other things. Let's get one thing out of the way first: this is the most complex and most powerful algorithm we'll be looking at, and offers the best results out of any of them. 

// demo

It's also faster than the two-color mix approach we've seen, scaling better with large palettes. A fast GPU implementation might even be able to run in real time as part of a graphics pipeline. How does it achieve all this?

This method is a little more complicated than the last one. Instead of picking two colors that make the best mix, we're going to find the best mix including any number of colors. The way we're going to do this is by adding colors to a *candidate list*, then choosing a color from that list based on the threshold map's value for that pixel.
```
code example
```

We can start by adding the obvious candidate, whichever color from our palette is closest to the original. Then, we can store error, and take it into account for the next color. Repeat until the list is full, and we have our candidate list.
```
code example
```

Note that it's possible for the same color to be added to the list several times, increasing the likelihood it will be selected.